<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://matdagommer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://matdagommer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-02T22:25:19+00:00</updated><id>https://matdagommer.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Bayes-UCB algorithm</title><link href="https://matdagommer.github.io/blog/2024/bayes-UCB-algorithm/" rel="alternate" type="text/html" title="Bayes-UCB algorithm"/><published>2024-06-13T19:37:00+00:00</published><updated>2024-06-13T19:37:00+00:00</updated><id>https://matdagommer.github.io/blog/2024/bayes-UCB-algorithm</id><content type="html" xml:base="https://matdagommer.github.io/blog/2024/bayes-UCB-algorithm/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/robot-casino-480.webp 480w,/assets/img/robot-casino-800.webp 800w,/assets/img/robot-casino-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/robot-casino.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Today, a colleague of mine presented a paper called <a href="https://www.nature.com/articles/s41586-024-07021-y">Identifying general reaction conditions by bandit optimization</a> during a Journal club. There are lots of things to say about the chemistry the authors are trying to tackle, but I had never heard of the algorithm before and had a hard time pretending I understood the paper as a result.</p> <p>Here’s a summary of what I learned about the Bayes-UCB algorithm.</p> <h3 id="reminder-of-multi-armed-bandit">Reminder of multi-armed bandit</h3> <p>The multi-armed bandit problem is a classic decision-making scenario where a gambler must choose between multiple slot machines (or “arms”), each with an unknown payout probability, to maximize their total reward over time. The challenge lies in balancing exploration (trying different arms to gather information about their payouts) and exploitation (selecting the arm known to yield the highest reward).</p> <p>This problem models real-world situations where one must make a series of decisions under uncertainty, such as clinical trials or online advertising. In the paper, the authors are looking for chemical reaction conditions (ligands, bases, activators, solvents, temperature…) that return the best yield across a batch of reactions. In their study, the arms are essentially different reaction conditions that we can choose from.</p> <h3 id="the-original-ucb-algorithm">the original UCB algorithm</h3> <p>Here’s the classic UCB algorithm (shamelessly stolen from these <a href="https://users.cs.duke.edu/~cynthia/CourseNotes/MABNotes.pdf">notes</a>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ucb-algo-480.webp 480w,/assets/img/ucb-algo-800.webp 800w,/assets/img/ucb-algo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ucb-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s dissect it:</p> <p>The number of rounds \(n\) is the total number of steps. The gambler is going to choose an arm \(n\) times. There are \(m\) arms. \(\hat{X}_{j, t}\) is the average reward measured for arm \(j\) at step \(t\). Initial values for each arm (\(\hat{X}_{1, 0}\), \(\hat{X}_{2, 0}\), …, \(\hat{X}_{m, 0}\)) are obtained by playing each arm once at the beginning. \(\hat{X}_{j, t-1}\) is the average reward measured for arm \(j\) at step \(t-1\). The sum \(\hat{X}_{j, t-1} + \sqrt{\frac{2log(t)}{T_j(t-1)}}\) is the mean value plus an exploration term.</p> <p>This is how the classic implementation manages the trade-off between exploration and exploitation:</p> <ul> <li>The mean reward takes care of the exploitation part.</li> <li>The \(T_j(t-1)\) term increases as you select the same arm. Naturally, the uncertainty on the average value decreases (the standard deviation of mean is equal to the standard deviation of the population divided by \(\sqrt{n}\) in the frequentist frame). For arms that haven’t been selected as many times, their uncertainty remains high which increases their chances to be picked up because the upper confidence bound remains high, which leaves space for exploration.</li> <li>In the exploration term, \(T_j(t-1)\) is the number of times arm \(j\) was selected in the past steps. It ensures that the arms that have been played the most have a lower exploration term, and that other arms are given higher priority.</li> </ul> <p>Note: some implementations use \(log(t-1)\) instead of \(2log(t)\). In the former, the UCB is “tighter” which reduces the exploration rate. This is known as the UCB1 variation.</p> <h3 id="bayes-ucb-algorithm">Bayes-UCB algorithm</h3> <p>In their paper <a href="https://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf">On Bayesian Upper Confidence Bounds for Bandit Problems</a>, Emilie Kaufmann, Olivier Cappé et Aurélien Garivier propose a Bayes version of the UCB algorithm. In a nutshell, the difference between the “frequentist” and the Bayesian approach lies in the definition of the mean reward. In the former, it is an unknown but fixed quantity that we estimate by computing an average and in the latter, a probability distribution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes-ucb-algo-480.webp 480w,/assets/img/bayes-ucb-algo-800.webp 800w,/assets/img/bayes-ucb-algo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/bayes-ucb-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Theorems and lemmas aside, here’s what we can learn by looking directly at the algorithm. The notation is different from the previous algorithm:</p> <p>The horizon \(n\) is also the total number of steps. \(\Pi^0\) is the initial prior on the set of the arms’ parameters \(\theta\). In the binary case, they are typically Beta distributions, and in the continuous case, Gaussian distributions (the Bayes-UCB paper uses uninformative priors but the chemistry paper uses an implementation with Gaussian priors). \(c\) designates mysterious “parameters of the quantile”.</p> <p>The double loop is equivalent to the for loop in the first algorithm: for each time step t, we are computing a quantity \(q_j(t)\) for each arm \(j\) and picking the arm that maximizes this quantity. Here also, the reward \(X_t\) is then sampled from the selected arm. This new data is used to update the posterior distribution \(\Pi^t\), and more specifically the distribution of arm j \(\pi_j^t\), since the arms are assumed to be independent from each other.</p> <p>Let’s look at \(q_j(t)\) in more detail:</p> <p>As explained in the paper, \(Q(\alpha, \rho)\) is the quantile function of the distribution \(\rho\). Here the distribution is \(\lambda_j^{t-1}\), the distribution of the arm’s mean reward before the update. We’re looking for the quantile at percentile \(1 - \frac1{t(log n)^c}\), which is another way of defining an Upper Confidence Bound of the current arm’s parameter distribution. The percentile \(1 - \frac1{t(log n)^c}\) is an “artifact of the theoretical analysis [..] But in simulations, the choice c = 0 actually proved to be the most satisfying”. Ok, we’ll assume that. Please replace \(1 - \frac1{t(log n)^c}\) by \(1 - \frac1{t}\) in your head :)</p> <p>So how is the exploitation-exploration managed in this new implementation?</p> <h3 id="bayesian-considerations">Bayesian considerations</h3> <p>The reward distribution \(\pi_{j,t}\) is updated using Bayes’ theorem: \(\pi_{j,t} (\theta_j) \propto \nu_{\theta_j}(X_t) \pi_{j,t} (\theta_j)\) where \(\nu_{\theta_j}(X_t)\) is the likelihood of observing reward \(X_t\) given mean \(\theta_j\).</p> <p>The distribution \(\lambda_{j,t}\) of the mean can be derived from the reward distribution \(\pi_{j,t}\) using Bayes’ theorem, since the mean is a parameter of the reward distribution:</p> <p>In the case of binary rewards (pass or fail), the reward can be modeled by a Bernoulli random variable with parameter \(\theta\). In order to save ourselves complexity, we can use a Beta distribution for parameter \(\theta\) as a conjugate prior such that \(\theta \sim Beta(a, b)\). The posterior becomes \(Beta(a + S_t(j), b + N_t(j) - S_t(j))\) where \(S_t(j)\) is the sum of rewards collected from that arm until step \(t\), and \(N_t(j)\) is the number of times arm \(j\) was picked up until step \(t\). The quantile is easily computed from this well-known distribution. In the chemistry paper, binary rewards are used for reactivity threshold (the selected conditions are either below or above that threshold).</p> <p>In the case of continuous reward, we can model it as a Gaussian distribution. Assuming a Gaussian prior, and assuming both the mean and variance of the prior are unknown, we can the following convenient conjugate priors: \(\mu | \sigma_0 \sim \mathcal{N}(\mu_0, \sigma^2/\kappa_0)\) and \(\sigma^2 \sim Inv-Gamma(\alpha_0, \beta_0)\). The resulting posterior is:</p> \[\begin{aligned} &amp; \mu \mid \sigma^2, \mathbf{x} \sim \mathcal{N}\left(\mu_n, \sigma^2 / \kappa_n\right) \\ &amp; \sigma^2 \mid \mathbf{x} \sim \operatorname{Inv}-\operatorname{Gamma}\left(\alpha_n, \beta_n\right) \end{aligned}\] <p>where the updated parameters are given by:</p> <p>\(\kappa_n=\kappa_0+n\), \(\mu_n=\frac{\kappa_0 \mu_0+n \bar{x}}{\kappa_0+n}\), \(\alpha_n=\alpha_0+\frac{n}{2}\) and \(\beta_n=\beta_0+\frac{1}{2}\left[\sum_{i=1}^n\left(x_i-\bar{x}\right)^2+\frac{\kappa_0 n\left(\bar{x}-\mu_0\right)^2}{\kappa_0+n}\right]\).</p> <p>Once again, all distributions are well-known and the quantiles are ready to be inferred.</p> <p>In the chemistry paper, the authors actually did something simpler and used a fixed variance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BayesUCBGaussianPPF</span><span class="p">(</span><span class="n">UCB1</span><span class="p">):</span>
    <span class="c1"># Used to be called NewBayesUCBGaussian
</span>    <span class="c1"># same as BayesUCBBetaPPF, but uses a gaussian prior with fixed variance
</span>
    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">'</span><span class="s">bayes_ucb_gaussian_ppf</span><span class="sh">'</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chosen_arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">RegretAlgorithm</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chosen_arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">counts</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ucbs</span> <span class="o">=</span> <span class="p">[</span><span class="n">norm</span><span class="p">.</span><span class="nf">ppf</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">counts</span><span class="p">)),</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">emp_means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)]</span>
</code></pre></div></div> <h3 id="quick-word-about-the-ucb1-tuned-algorithm">Quick word about the UCB1-tuned algorithm:</h3> <p>This algorithm is also being used in the paper and is a variation of the UCB algorithm. Contrary to UCB and Bayes-UCB, it takes into account the measured variance. The UCB1-Tuned has also the advantage of not requiring user-defined parameters, although being slightly less performant as the Bayes-UCB.</p> <p>Here’s an extract from the original paper:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ucb1-tuned-algo-480.webp 480w,/assets/img/ucb1-tuned-algo-800.webp 800w,/assets/img/ucb1-tuned-algo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ucb1-tuned-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="sources">Sources:</h4> <p>https://www.nature.com/articles/s41586-024-07021-y</p> <p>https://github.com/doyle-lab-ucla/bandit-optimization</p> <p>https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d</p> <p>https://link.springer.com/article/10.1023/A:1013689704352</p> <p>https://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf</p> <p>https://users.cs.duke.edu/~cynthia/CourseNotes/MABNotes.pdf</p> <p>https://webdocs.cs.ualberta.ca/~games/go/seminar/notes/2007/slides_ucb.pdf</p>]]></content><author><name></name></author><category term="ML"/><category term="Bayes,"/><category term="ML,"/><category term="Algorithm,"/><category term="Multi-armed"/><category term="bandit,"/><category term="Optimization"/><summary type="html"><![CDATA[Optimization for real G's.]]></summary></entry><entry><title type="html">Gaussian Processes</title><link href="https://matdagommer.github.io/blog/2024/gaussian-processes/" rel="alternate" type="text/html" title="Gaussian Processes"/><published>2024-05-24T22:24:00+00:00</published><updated>2024-05-24T22:24:00+00:00</updated><id>https://matdagommer.github.io/blog/2024/gaussian-processes</id><content type="html" xml:base="https://matdagommer.github.io/blog/2024/gaussian-processes/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gaussian-processes-480.webp 480w,/assets/img/gaussian-processes-800.webp 800w,/assets/img/gaussian-processes-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gaussian-processes.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="introduction">Introduction</h3> <p>My recent reflection on uncertainty quantification in machine learning led me to take a closer look at Gaussian Processes. Turns out that Gaussian processes inherently allow us to compute some kind of uncertainty!</p> <h3 id="bayesian-perspective-on-weight-derivation">Bayesian Perspective on Weight Derivation</h3> <p>What most machine learning models do is come up with a set of weights or a model we can use to do inference. Usually, the way these weights are derived is by looking at the Maximum A Posteriori (MAP), which means they maximize the posterior distribution. If you look at the Bayesian interpretation of ordinary least squares or ridge regression, it turns out that it is equivalent to finding the weights that maximize the weights’ posterior distribution.</p> \[\hat{\theta} = \arg\max_\theta P(\theta | X, Y) = \arg\max_\theta P(Y | X, \theta) P(\theta)\] <h3 id="posterior-predictive-distribution">Posterior Predictive Distribution</h3> <p>But the problem with this approach is that you only consider a single set of weights for your predictions. And in the process, you ruled out completely other possible models, although they may be as valid! Gaussian processes allow us to take into account all model configurations by directly modeling the posterior predictive distribution. Indeed, if we look at it from the Bayesian perspective, we see that the posterior predictive distribution integrates over all possible sets of model weights:</p> \[P(Y^* | X^*, X, Y) = \int_{\theta} P(Y^* | X^*, \theta) P(\theta | X, Y) d\theta\] <h3 id="assumption-of-gaussian-distributions">Assumption of Gaussian Distributions</h3> <p>The idea behind Gaussian processes is to assume that the posterior predictive distribution is a multivariate Gaussian. This <a href="https://www.youtube.com/watch?v=R-NUdqxKjos">video</a> explains nicely why this assumption is reasonable. Here is the short version: if your prior and your likelihood distributions are assumed to be Gaussian, then the posterior distribution becomes Gaussian immediately (because Gaussian distributions are conjugate prior of… Gaussian distributions).</p> \[P(Y | X) \sim \mathcal{N}(\mu, \Sigma)\] <h3 id="covariance-matrix-and-kernel-trick">Covariance Matrix and Kernel Trick</h3> <p>The Gaussian distribution assumption is reasonable and we’ll go on with it. Now here’s the trick: not only do we assume that the training set follows a Gaussian distribution, we also assume that the combination of the training points and the test points also follow a multivariate Gaussian distribution:</p> \[\begin{bmatrix} Y \\ Y^* \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \mu \\ \mu^* \end{bmatrix}, \Sigma \right)\] <p>What should our Gaussian distribution parameters (mean values and covariance matrix) be? It turns out we don’t really care about the mean values. The simple, practical explanation is that we can center the data around zero as a preprocessing step. Regarding the covariance matrix, we can use a cool trick: a kernel! Essentially, a kernel enables us to define a surrogate for the covariance matrix which is more a “similarity matrix” than it is a covariance matrix.</p> \[\Sigma = \begin{bmatrix} K_{X, X} &amp; K_{X, X^*} \\ K_{X, X^*}^T &amp; K_{X^*, X^*} \end{bmatrix}\] <p>where \(K_{X, X}\) is the train covariance matrix, \(K_{X, X^*}\) is the train-test covariance matrix and \(K_{X^*, X^*}\) the test covariance matrix.</p> <p>The RBF kernel is commonly used and is a way to capture the similarity between two data points:</p> \[k(x, x') = \sigma_f^2\exp\left(-\frac{\|x - x'\|^2}{2l}\right)\] <p>Note that this expression can be generalized when the input space has several dimensions: \(k(x, x') = \sigma_f^2\exp\left(-\frac1{2l}(\textbf{x} - \textbf{x'})^T(\textbf{x} - \textbf{x'})\right)\)</p> <p>“But why the heck are we customizing the covariance matrix?”</p> <p>Well, there is no way we can come up with a meaningful covariance matrix given our current dataset. What would it even mean to compute a variance between \(y_1\) and \(y_n\)? They’re single observations! However, we can use a kernel function to compute the similarity between the corresponding input values. And in return, these can lead to a surrogate to the covariance matrix in the joint distribution that captures this simple idea: if two input values \(x\) and \(x'\) are similar, then the associated output values \(y\) and \(y'\) should be similar too! The best way to grasp this is to take a look at a slice from a 2D-Gaussian distribution where the two components are correlated. If one moves, the other moves too, hence the high “covariance”.</p> <p>By choosing an appropriate kernel function that correctly captures the similarity between input values, we should retrieve a nice joint distribution. Now, we need to derive the conditional distribution of the test point we want to infer. Here’s the formula, I’m sparing myself the math:</p> \[P(Y^* | X^*, X, Y) \sim \mathcal{N}(K_{X^*, X} K_{X, X}^{-1} Y, K_{X^*, X^*} - K_{X^*, X} K_{X, X}^{-1} K_{X, X^*})\] <h3 id="some-intuition">Some intuition</h3> <p>What’s important to understand at this point is that candidate “functions” that model our data can be sampled from the joint distribution. I used quote marks here because we don’t retrieve a function per se, but rather a set of what the true values could look like for the different \(x\) values. If I have 20 points in my training set, I am basically sampling one vector from a 20-dimnensional Gaussian distribution. This is well explained in this short <a href="https://arxiv.org/pdf/2009.10862">paper</a>.</p> <p>In addition, the joint distribution and its kernel constitute a prior (“kernelized prior function”), and the conditional distribution is the posterior distribution we obtain by acknowledging the training data (\(Y\) vector).</p> <h3 id="hyperparameter-optimization">Hyperparameter Optimization</h3> <p>The kernel function we introduced has two parameters \(\sigma_f\) and \(l\), respectively the vertical and horizontal scale. I’ll designated them under the term \(\beta\). They can be optimized by maximizing the log marginal likelihood \(\log P(Y \| X, \beta)\) (it’s almost the same multivariate distribution we used earlier, but without the test points):</p> \[\beta^* = \arg\max_\beta \log P(Y|X,\beta) = \arg\max_\beta -\frac1{2}Y^TK^{-1}Y - \frac{n}{2}\log2\pi -\frac1{2}\log|K|\] <h3 id="conclusion">Conclusion</h3> <p>This is only the beginning of my exploration with Gaussian processes, and I’m eager to learn more about the applications. My understanding is that Gaussian processes are well suited for small datasets but scale badly with big datasets, especially because matrix inversion has a computation time complexity of \(O(n^3)\). However, I am really hyped about the uncertainty measure we can get out of it.</p> <p><strong>Sources</strong></p> <p>https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html</p> <p>https://scikit-learn.org/stable/modules/gaussian_process.html</p> <p>https://en.wikipedia.org/wiki/Gaussian_process</p> <p>https://www.youtube.com/watch?v=UBDgSHPxVME</p> <p>https://arxiv.org/pdf/2009.10862</p>]]></content><author><name></name></author><category term="ML"/><category term="Bayesian"/><category term="Statistics,"/><category term="ML,"/><category term="Uncertainty"/><category term="Quantification"/><summary type="html"><![CDATA[A quick look at Gaussian Processes.]]></summary></entry><entry><title type="html">Decision trees in 6 minutes</title><link href="https://matdagommer.github.io/blog/2024/decision-trees/" rel="alternate" type="text/html" title="Decision trees in 6 minutes"/><published>2024-04-25T15:09:00+00:00</published><updated>2024-04-25T15:09:00+00:00</updated><id>https://matdagommer.github.io/blog/2024/decision-trees</id><content type="html" xml:base="https://matdagommer.github.io/blog/2024/decision-trees/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>After 2 years of doing projects in the machine learning space, I realized I didn’t know how decision tree models actually worked. I never felt the urge of learning the inner working for 2 reasons:</p> <ul> <li>My early ML classes focused on differentiable models (Andrew Ng).</li> <li>Implementation of decision trees with the Python scikit-learn, is very easy and high-level.</li> </ul> <p>So, I decided to take a good look! This short post contains a quick description of the training process of a decision tree in a language my future self will understand, and hopefully you too.</p> <p><strong>I. The tree structure</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/decision_tree-480.webp 480w,/assets/img/decision_tree-800.webp 800w,/assets/img/decision_tree-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/decision_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I’m sure you’ve encountered tree structures before, and they’re fairly easy to make sense of when presented in that format: take your input, and depending on its features (alcohol content, sulfur dioxide content), follow the path. Boom. Your output (in this case, your wine quality) is predicted to be a 6 (good) or a 5 (less good). Straightforward.</p> <p>But this tree is just one tree among a myriad of possibilities, and I could come up with an infinity of combinations: why not sort wines based on their density instead of their alcohol content at the root node? Why not change the threshold to 2.120 instead of 10.525? What decided that structure and these specific sequence of features and thresholds? Let’s break it down:</p> <p>At every node (nodes represent stages at which data gets sorted) of the tree, we want to separate the training data into different groups in a fashion that allows us to retrieve more homogeneous subsets (in terms of their target variable) at every step. In the case of classification (resp. regression), this means that training points with similar labels (resp., target values) tend to get sorted in the same subset.</p> <p>In order to determine what feature will be used to separate the data, and what threshold will be used, the algorithm loops through all possible (feature, threshold) combinations, computes a criterion metric (information gain, Gini impurity, variance reduction) every time and picks the combination that optimizes that metric. This criterion metrics assess how much the separation homogenized the new subsets with respect to the initial one.</p> <p>Let’s take the information gain as an example:</p> <p>Without diving into too much details, the information gain is a metric used in classification problems, which works well with the wine quality dataset (wines are classified into 6 quality categories: 3, 4, 5, 6, 7, and 8). If you’re a bit familiar with Shannon’s entropy (\(H = - \sum_{i} p_i \log_{2}(p_i)\)), it’s fairly easy to understand:</p> \[IG = H_X - H_{X_1} - H_{X_2}\] <p>where \(X\), \(X_1\) and \(X_2\) represent the initial dataset, first and second subsets. We gain information when the entropies of subsets 1 and 2 are lower than the entropy of the dataset before separation. In other words, the distribution of points in these subsets tends to be more concentrated around one class compared to the previous dataset’s distribution.</p> <p>So with information gain, we have a quantitative ways of assessing what (feature, threshold) combination returns the most informative separation at each node. I leave you with this intuition, but I encourage you to check the math behind the other metrics as well!</p> <p>If you keep doing that process at every node, you’ll end up with a beautiful tree structure. Now, there are hyperparameters you can adjust to restrict the arborescence of the tree and avoid overfitting. Setting a maximum depth, and setting a minimum information gain are ways to go.</p> <p><strong>II. Inference</strong></p> <p>Inference consist in running your sample from the root node to until it reaches terminal node. For classification, we typically classify the sample in the class that is the most represented in the corresponding training subset. If the majority of training points are from class 6, the test point gets attributed the class 6. For regression, we would take the average of training points’ target value as the predicted value.</p> <p>In the wine quality dataset, features are continuous (alcohol content, total sulfur dioxide content…). But we could have features that are categorical (e.g., color). In that case, we would loop through every (feature, feature category) combinations.</p> <p>This is the code I used to generate the figure:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Load the dataset
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv</span><span class="sh">"</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="sh">'</span><span class="s">;</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Separate features and target variable
</span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">quality</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">'</span><span class="s">quality</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Split the dataset into training and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a decision tree classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Plot the decision tree
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">tree</span><span class="p">.</span><span class="nf">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">3</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">4</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">6</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">7</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">8</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure> <p><strong>Sources:</strong></p> <p>https://en.wikipedia.org/wiki/Decision_tree_learning</p> <p>https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6</p>]]></content><author><name></name></author><category term="ML"/><category term="Decision"/><category term="Tree,"/><category term="ML,"/><category term="Random"/><category term="Forest,"/><category term="XGBoost"/><summary type="html"><![CDATA[Quick notes on decision trees training.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://matdagommer.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://matdagommer.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://matdagommer.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>
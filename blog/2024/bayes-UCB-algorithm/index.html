<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bayes-UCB algorithm | Matthieu Dagommer </title> <meta name="author" content="Matthieu Dagommer"> <meta name="description" content="Optimization for real G's."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://matdagommer.github.io/blog/2024/bayes-UCB-algorithm/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Matthieu </span> Dagommer </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bayes-UCB algorithm</h1> <p class="post-meta"> June 13, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/bayes"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayes,</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML,</a>   <a href="/blog/tag/algorithm"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithm,</a>   <a href="/blog/tag/multi-armed"> <i class="fa-solid fa-hashtag fa-sm"></i> Multi-armed</a>   <a href="/blog/tag/bandit"> <i class="fa-solid fa-hashtag fa-sm"></i> bandit,</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>     ·   <a href="/blog/category/ml"> <i class="fa-solid fa-tag fa-sm"></i> ML</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/robot-casino-480.webp 480w,/assets/img/robot-casino-800.webp 800w,/assets/img/robot-casino-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/robot-casino.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Today, a colleague of mine presented a paper called <a href="https://www.nature.com/articles/s41586-024-07021-y" rel="external nofollow noopener" target="_blank">Identifying general reaction conditions by bandit optimization</a> during a Journal club. There are lots of things to say about the chemistry the authors are trying to tackle, but I had never heard of the algorithm before and had a hard time pretending I understood the paper as a result.</p> <p>Here’s a summary of what I learned about the Bayes-UCB algorithm.</p> <h3 id="reminder-of-multi-armed-bandit">Reminder of multi-armed bandit</h3> <p>The multi-armed bandit problem is a classic decision-making scenario where a gambler must choose between multiple slot machines (or “arms”), each with an unknown payout probability, to maximize their total reward over time. The challenge lies in balancing exploration (trying different arms to gather information about their payouts) and exploitation (selecting the arm known to yield the highest reward).</p> <p>This problem models real-world situations where one must make a series of decisions under uncertainty, such as clinical trials or online advertising. In the paper, the authors are looking for chemical reaction conditions (ligands, bases, activators, solvents, temperature…) that return the best yield across a batch of reactions. In their study, the arms are essentially different reaction conditions that we can choose from.</p> <h3 id="the-original-ucb-algorithm">the original UCB algorithm</h3> <p>Here’s the classic UCB algorithm shamelessly stolen from these <a href="https://users.cs.duke.edu/~cynthia/CourseNotes/MABNotes.pdf" rel="external nofollow noopener" target="_blank">notes</a>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ucb-algo-480.webp 480w,/assets/img/ucb-algo-800.webp 800w,/assets/img/ucb-algo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ucb-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Let’s dissect it:</p> <p>The number of rounds \(n\) is the total number of steps. The gambler is going to choose an arm \(n\) times. There are \(m\) arms. \(\hat{X}_{j, t}\) is the average reward measured for arm \(j\) at step \(t\). Initial values for each arm (\(\hat{X}_{1, 0}\), \(\hat{X}_{2, 0}\), …, \(\hat{X}_{m, 0}\)) are obtained by playing each arm once at the beginning. \(\hat{X}_{j, t-1}\) is the average reward measured for arm \(j\) at step \(t-1\). The sum \(\hat{X}_{j, t-1} + \sqrt(frac{2log(t)}{T_j(t-1)})\) is the mean value plus an exploration term.</p> <p>This is how the classic implementation manages the trade-off between exploration and exploitation:</p> <ul> <li>The mean reward takes care of the exploitation part.</li> <li>The \(T_j(t-1)\) term increases as you select the same arm. Naturally, the uncertainty on the average value decreases (the standard deviation of mean is equal to the standard deviation of the population divided by \(\sqrt(n)\) in the frequentist frame). For arms that haven’t been selected as many times, their uncertainty remains high which increases their chances to be picked up because the upper confidence bound remains high, which leaves space for exploration.</li> <li>In the exploration term, \(T_j(t-1)\) is the number of times arm \(j\) was selected in the past steps. It ensures that the arms that have been played the most have a lower exploration term, and that other arms are given higher priority.</li> </ul> <p>Note: some implementations use \(log(t-1)\) instead of \(2log(t)\). In the former, the UCB is “tighter” which reduces the exploration rate. This is known as the UCB1 variation.</p> <h3 id="bayes-ucb-algorithm">Bayes-UCB algorithm</h3> <p>In their paper <a href="https://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf" rel="external nofollow noopener" target="_blank">On Bayesian Upper Confidence Bounds for Bandit Problems</a>, Emilie Kaufmann, Olivier Cappé et Aurélien Garivier propose a Bayes version of the UCB algorithm. In a nutshell, the difference between the “frequentist” and the Bayesian approach lies in the definition of the mean reward. In the former, it is an unknown but fixed quantity that we estimate by computing an average and in the latter, a probability distribution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes-ucb-algo-480.webp 480w,/assets/img/bayes-ucb-algo-800.webp 800w,/assets/img/bayes-ucb-algo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/bayes-ucb-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Theorems and lemmas aside, here’s what we can learn by looking directly at the algorithm. The notation is different from the previous algorithm:</p> <p>The horizon \(n\) is also the total number of steps. \(\Pi^0\) is the initial prior on the set of the arms’ parameters \(\theta\). In the binary case, they are typically Beta distributions, and in the continuous case, Gaussian distributions (the Bayes-UCB paper uses uninformative priors but the chemistry paper uses an implementation with Gaussian priors). \(c\) designates mysterious “parameters of the quantile”.</p> <p>The double loop is equivalent to the for loop in the first algorithm: for each time step t, we are computing a quantity \(q_j(t)\) for each arm \(j\) and picking the arm that maximizes this quantity. Here also, the reward \(X_t\) is then sampled from the selected arm. This new data is used to update the posterior distribution \(Pi^t\), and more specifically the distribution of arm j \(pi_j^t\), since the arms are assumed to be independent from each other.</p> <p>Let’s look at \(q_j^t\) in more detail:</p> <p>As explained in the paper, \(Q(\alpha, \rho)\) is the quantile function of the distribution \rho. Here the distribution is \(\lambda_j^{t-1}\), the distribution of the arm’s mean reward before the update. We’re looking for the quantile at percentile \(1 - \frac1{t(log n)^c}\), which is another way of defining an Upper Confidence Bound of the current arm’s parameter distribution. The percentile \(1 - \frac1{t(log n)^c}\) is an “artifact of the theoretical analysis [..] But in simulations, the choice c = 0 actually proved to be the most satisfying”. Ok, I’ll assume that. Replace \(1 - \frac1{t(log n)^c}\) by \(1 - \frac1{t}\)!</p> <p>So how is the exploitation-exploration managed in this new implementation?</p> <h3 id="bayesian-considerations">Bayesian considerations</h3> <p>The reward distribution \(\pi_j,t\) is updated using Bayes’ theorem: \(\pi_j,t (\theta_j) \propto \nu_{\theta_j}(X_t) \pi_j,t (\theta_j)\) where \(\nu_{\theta_j}(X_t)\) is the likelihood of observing reward X_t given mean \(theta_j\).</p> <p>The distribution \(\lambda_{j,t}\) of the mean can be derived from the reward distribution \(\pi_{j,t}\) using Bayes’ theorem, since the mean is a parameter of the reward distribution:</p> <ul> <li> <p>In the case of binary rewards (pass or fail), the reward can be modeled by a Bernoulli random variable with parameter \(\theta\). In order to save ourselves complexity, we can use a Beta distribution for parameter \(\theta\) as a conjugate prior such that \(\theta ~ Beta(a, b)\). The posterior becomes \(Beta(a + S_t(j), b + N_t(j) - S_t(j))\) where \(S_t(j)\) is the sum of rewards collected from that arm until step \(t\). The quantile is easily computed from this well-known distribution. In the chemistry paper, binary rewards are used for reactivity threshold (the selected conditions are either below or above that threshold).</p> </li> <li> <p>In the case of continuous reward, we can model it as a Gaussian distribution. Assuming a Gaussian prior, and assuming both the mean and variance of the prior are unknown, we can the following convenient conjugate priors: \(\mu | \sigma_0 ~ \mathcal{N}(\mu_0, \sigma^2/\kappa_0)\) and \(\sigma^2 ~ Inv-Gamma(\alpha_0, \beta_0)\). The resulting posterior is:</p> </li> </ul> \[\begin{aligned} &amp; \mu \mid \sigma^2, \mathbf{x} \sim \mathcal{N}\left(\mu_n, \sigma^2 / \kappa_n\right) \\ &amp; \sigma^2 \mid \mathbf{x} \sim \operatorname{Inv}-\operatorname{Gamma}\left(\alpha_n, \beta_n\right) \end{aligned}\] <p>where the updated parameters are given by:</p> <ul> <li> \[\kappa_n=\kappa_0+n\] </li> <li> \[\mu_n=\frac{\kappa_0 \mu_0+n \bar{x}}{\kappa_0+n}\] </li> <li> \[\alpha_n=\alpha_0+\frac{n}{2}\] </li> <li> <p>\(\beta_n=\beta_0+\frac{1}{2}\left[\sum_{i=1}^n\left(x_i-\bar{x}\right)^2+\frac{\kappa_0 n\left(\bar{x}-\mu_0\right)^2}{\kappa_0+n}\right]\) Once again, all distributions are well-known and the quantiles are ready to be inferred.</p> </li> <li>In the chemistry paper, the authors actually did something simpler and used a fixed variance:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BayesUCBGaussianPPF</span><span class="p">(</span><span class="n">UCB1</span><span class="p">):</span>
    <span class="c1"># Used to be called NewBayesUCBGaussian
</span>    <span class="c1"># same as BayesUCBBetaPPF, but uses a gaussian prior with fixed variance
</span>
    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="sh">'</span><span class="s">bayes_ucb_gaussian_ppf</span><span class="sh">'</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chosen_arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="n">RegretAlgorithm</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chosen_arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">stds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">counts</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ucbs</span> <span class="o">=</span> <span class="p">[</span><span class="n">norm</span><span class="p">.</span><span class="nf">ppf</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">counts</span><span class="p">)),</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">emp_means</span><span class="p">,</span> <span class="n">stds</span><span class="p">)]</span>
</code></pre></div></div> <h3 id="quick-word-about-the-ucb1-tuned-algorithm">Quick word about the UCB1-tuned algorithm:</h3> <p>This algorithm is also being used in the paper and is a variation of the UCB algorithm. It has the advantage of being non-parametric, although being slightly less performant as the Bayes-UCB.</p> <p>Here’s an extract from the original paper:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ucb1-tuned-algo-480.webp 480w,/assets/img/ucb1-tuned-algo-800.webp 800w,/assets/img/ucb1-tuned-algo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ucb1-tuned-algo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="sources">Sources:</h4> <p>https://www.nature.com/articles/s41586-024-07021-y</p> <p>https://github.com/doyle-lab-ucla/bandit-optimization</p> <p>https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d</p> <p>https://link.springer.com/article/10.1023/A:1013689704352</p> <p>https://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf</p> <p>https://users.cs.duke.edu/~cynthia/CourseNotes/MABNotes.pdf</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/gaussian-processes/">Gaussian Processes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/decision-trees/">Decision trees in 5 minutes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Matthieu Dagommer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>